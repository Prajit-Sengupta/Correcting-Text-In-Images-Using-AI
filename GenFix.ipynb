{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./genai/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in ./genai/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: transformers in ./genai/lib/python3.10/site-packages (4.48.3)\n",
      "Requirement already satisfied: diffusers in ./genai/lib/python3.10/site-packages (0.32.2)\n",
      "Requirement already satisfied: numpy in ./genai/lib/python3.10/site-packages (1.23.5)\n",
      "Requirement already satisfied: easyocr in ./genai/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: scipy in ./genai/lib/python3.10/site-packages (1.15.1)\n",
      "Requirement already satisfied: networkx in ./genai/lib/python3.10/site-packages (3.4.2)\n",
      "Requirement already satisfied: pillow in ./genai/lib/python3.10/site-packages (11.1.0)\n",
      "Requirement already satisfied: filelock in ./genai/lib/python3.10/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./genai/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in ./genai/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./genai/lib/python3.10/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./genai/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./genai/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./genai/lib/python3.10/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./genai/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./genai/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./genai/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./genai/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./genai/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./genai/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./genai/lib/python3.10/site-packages (from transformers) (4.65.2)\n",
      "Requirement already satisfied: importlib-metadata in ./genai/lib/python3.10/site-packages (from diffusers) (8.6.1)\n",
      "Requirement already satisfied: opencv-python-headless in ./genai/lib/python3.10/site-packages (from easyocr) (4.11.0.86)\n",
      "Requirement already satisfied: scikit-image in ./genai/lib/python3.10/site-packages (from easyocr) (0.25.1)\n",
      "Requirement already satisfied: python-bidi in ./genai/lib/python3.10/site-packages (from easyocr) (0.6.3)\n",
      "Requirement already satisfied: Shapely in ./genai/lib/python3.10/site-packages (from easyocr) (2.0.7)\n",
      "Requirement already satisfied: pyclipper in ./genai/lib/python3.10/site-packages (from easyocr) (1.3.0.post6)\n",
      "Requirement already satisfied: ninja in ./genai/lib/python3.10/site-packages (from easyocr) (1.11.1.3)\n",
      "Requirement already satisfied: zipp>=3.20 in ./genai/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./genai/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./genai/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./genai/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./genai/lib/python3.10/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./genai/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.3-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in ./genai/lib/python3.10/site-packages (from scikit-image->easyocr) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./genai/lib/python3.10/site-packages (from scikit-image->easyocr) (2025.1.10)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in ./genai/lib/python3.10/site-packages (from scikit-image->easyocr) (0.4)\n",
      "Downloading numpy-2.2.3-cp310-cp310-macosx_14_0_arm64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "albucore 0.0.13 requires numpy<2,>=1.24.4, but you have numpy 2.2.3 which is incompatible.\n",
      "albumentations 1.4.10 requires numpy<2,>=1.24.4, but you have numpy 2.2.3 which is incompatible.\n",
      "paddleocr 2.9.1 requires numpy<2.0, but you have numpy 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers diffusers numpy easyocr scipy networkx pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6b1d31a87d40f0bd9413853e97abf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.48.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 1024,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e32430f70249b382c0cbb7caa36eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d726cc136ed41f882955ceecb43e1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, BlipProcessor, BlipForConditionalGeneration\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import networkx as nx\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "\n",
    "class AITextCorrector:\n",
    "    def __init__(self, trocr_model=\"microsoft/trocr-large-handwritten\",\n",
    "                 blip_model=\"Salesforce/blip-image-captioning-base\",\n",
    "                 model_name=\"stabilityai/stable-diffusion-2-inpainting\"):\n",
    "        \"\"\"\n",
    "        Initialize models: TrOCR for OCR, BLIP for context-aware correction, and TextDiffuser for inpainting.\n",
    "        \"\"\"\n",
    "        # if torch.backends.mps.is_available():\n",
    "        #     self.device = torch.device(\"mps\")  # Use Apple MPS\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")  # Use CUDA if available (not applicable for Macs)\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")  # Default to CPU\n",
    "\n",
    "        print(\"Using device:\", self.device)\n",
    "\n",
    "        # OCR - TrOCR\n",
    "        self.ocr_processor = TrOCRProcessor.from_pretrained(trocr_model)\n",
    "        # self.ocr_model = VisionEncoderDecoderModel.from_pretrained(trocr_model).to(self.device)\n",
    "\n",
    "        # First, load the model\n",
    "        self.ocr_model = VisionEncoderDecoderModel.from_pretrained(trocr_model)\n",
    "\n",
    "        # Then, infer the device map\n",
    "        device_map = infer_auto_device_map(self.ocr_model)\n",
    "\n",
    "        # Finally, move the model to the computed device\n",
    "        self.ocr_model = dispatch_model(self.ocr_model, device_map=device_map)\n",
    "\n",
    "        # Captioning - BLIP-2\n",
    "        self.blip_processor = BlipProcessor.from_pretrained(blip_model)\n",
    "        self.blip_model = BlipForConditionalGeneration.from_pretrained(blip_model).to(self.device)\n",
    "\n",
    "        # Text Inpainting - TextDiffuser\n",
    "        self.model = StableDiffusionInpaintPipeline.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "        # Traditional OCR for Bounding Box Detection\n",
    "        self.easyocr_model = easyocr.Reader(['en'])\n",
    "\n",
    "    def detect_text_boxes(self, image):\n",
    "        \"\"\"\n",
    "        Detects text regions using EasyOCR and extracts bounding boxes.\n",
    "        \"\"\"\n",
    "        image_np = np.array(image)\n",
    "        ocr_results = self.easyocr_model.readtext(image_np)\n",
    "        return [{\"coordinates\": result[0], \"text\": result[1]} for result in ocr_results]\n",
    "\n",
    "    def recognize_text(self, image):\n",
    "        \"\"\"\n",
    "        Recognizes text in the image using TrOCR.\n",
    "        \"\"\"\n",
    "        pixel_values = self.ocr_processor(images=image, return_tensors=\"pt\").pixel_values.to(self.device)\n",
    "        generated_ids = self.ocr_model.generate(pixel_values)\n",
    "        return self.ocr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    def generate_caption(self, image):\n",
    "        \"\"\"\n",
    "        Generates a descriptive caption for the image using BLIP-2.\n",
    "        \"\"\"\n",
    "        inputs = self.blip_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs[\"pixel_values\"]  # Extract pixel values\n",
    "        with torch.no_grad():\n",
    "            outputs = self.blip_model.generate(pixel_values=pixel_values)  # Pass pixel_values explicitly\n",
    "        return self.blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def correct_text(self, extracted_text, caption, image):\n",
    "        \"\"\"\n",
    "        Uses BLIP-2 to refine extracted text based on image caption context.\n",
    "        \"\"\"\n",
    "        inputs = self.blip_processor(images=image, text=f\"Correct this text: {extracted_text} in context: {caption}\", return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs[\"pixel_values\"]  # Extract pixel values\n",
    "        with torch.no_grad():\n",
    "            outputs = self.blip_model.generate(pixel_values=pixel_values)  # ✅ Pass explicitly\n",
    "        return self.blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    def create_mask(self, image_size, coordinates):\n",
    "        \"\"\"\n",
    "        Creates a binary mask for the text regions.\n",
    "        \"\"\"\n",
    "        mask = Image.new('L', image_size, 0)\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "        draw.polygon([tuple(point) for point in coordinates], outline=255, fill=255)\n",
    "        return mask\n",
    "\n",
    "    def graph_based_text_alignment(self, detected_boxes):\n",
    "        \"\"\"\n",
    "        Uses a graph-based Hungarian Matching algorithm to align detected text positions.\n",
    "        \"\"\"\n",
    "        num_boxes = len(detected_boxes)\n",
    "        cost_matrix = np.zeros((num_boxes, num_boxes))\n",
    "\n",
    "        for i in range(num_boxes):\n",
    "            for j in range(num_boxes):\n",
    "                if i != j:\n",
    "                    # Distance-based cost function\n",
    "                    x1, y1 = np.mean(detected_boxes[i]['coordinates'], axis=0)\n",
    "                    x2, y2 = np.mean(detected_boxes[j]['coordinates'], axis=0)\n",
    "                    cost_matrix[i, j] = np.linalg.norm(np.array([x1, y1]) - np.array([x2, y2]))\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        aligned_boxes = [detected_boxes[i] for i in row_ind]\n",
    "        return aligned_boxes\n",
    "\n",
    "    def inpaint_text(self, image, mask, corrected_text):\n",
    "        \"\"\"\n",
    "        Inpaints the corrected text using TextDiffuser.\n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "        return self.model(prompt=f\"Generate text '{corrected_text}' in a matching style\", image=image, mask_image=mask, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
    "\n",
    "    def run_pipeline(self, image):\n",
    "        \"\"\"\n",
    "        Runs the complete text correction pipeline.\n",
    "        \"\"\"\n",
    "        text_boxes = self.detect_text_boxes(image)\n",
    "        caption = self.generate_caption(image)\n",
    "        aligned_boxes = self.graph_based_text_alignment(text_boxes)\n",
    "\n",
    "        corrected_image = image.copy()\n",
    "\n",
    "        for box in aligned_boxes:\n",
    "            original_text = box[\"text\"]\n",
    "            corrected_text = self.correct_text(original_text, caption, image)\n",
    "\n",
    "            if corrected_text.strip() == original_text.strip():\n",
    "                continue  # Skip if no correction needed\n",
    "            \n",
    "            # mask = self.create_mask(image.size, box[\"coordinates\"])\n",
    "            \n",
    "            mask = self.create_mask(image.size, box[\"coordinates\"])\n",
    "            mask = mask.resize(image.size)  # Ensure mask is the same size as image\n",
    "            inpainted_region = self.inpaint_text(corrected_image, mask, corrected_text)\n",
    "\n",
    "            # Blend the corrected text back into the image\n",
    "            full_mask = Image.new('L', corrected_image.size, 0)\n",
    "            full_mask.paste(mask, (0, 0))\n",
    "            # corrected_image.paste(inpainted_region, (0, 0), full_mask)\n",
    "            # Ensure the mask is resized to match the original image\n",
    "            mask = mask.resize(image.size, Image.LANCZOS)\n",
    "\n",
    "            # Ensure the inpainted region is the same size as the mask\n",
    "            inpainted_region = inpainted_region.resize(image.size, Image.LANCZOS)\n",
    "\n",
    "            # Debugging: Print sizes before pasting\n",
    "            print(\"Original image size:\", image.size)\n",
    "            print(\"Mask size:\", mask.size)\n",
    "            print(\"Inpainted region size:\", inpainted_region.size)\n",
    "\n",
    "            # Paste the inpainted region back into the corrected image\n",
    "            corrected_image.paste(inpainted_region, (0, 0), mask)\n",
    "\n",
    "        return corrected_image\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    corrector = AITextCorrector()\n",
    "\n",
    "    input_image = Image.open(\"GenAI_Dataset/Dall-E3(ChatGpt)/DALL·E 2025-02-15 00.29.46 - A breathtaking night sky filled with countless stars, stretching across the horizon. The Milky Way is visible, creating a stunning cosmic backdrop. In.webp\")  # Replace with your test image\n",
    "    output_image = corrector.run_pipeline(input_image)\n",
    "    print(\"Corrected Image Generated\")\n",
    "    output_image.save(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free unused memory on MPS\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Run garbage collection to free memory\n",
    "gc.collect()\n",
    "\n",
    "# Free unused memory on MPS\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Levenshtein distance\n",
    "!pip install python-Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9e3498f19647f2af082263c7d1dba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc64d339b4564211b871b24d444d83e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize, linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "import easyocr\n",
    "\n",
    "class EnhancedTextCorrector:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Initializing models on {self.device}\")\n",
    "        \n",
    "        # Initialize OCR reader with GPU acceleration\n",
    "        self.easyocr_reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
    "        \n",
    "        # Initialize BLIP-2 for contextual understanding\n",
    "        self.blip_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "        self.blip_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/blip2-opt-2.7b\", \n",
    "            torch_dtype=torch.float16 if 'cuda' in str(self.device) else torch.float32\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Initialize inpainting pipeline with memory optimization\n",
    "        self.inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-2-inpainting\",\n",
    "            torch_dtype=torch.float16 if 'cuda' in str(self.device) else torch.float32\n",
    "        ).to(self.device)\n",
    "        self.inpaint_pipe.enable_attention_slicing()\n",
    "\n",
    "    def detect_text_regions(self, image):\n",
    "        \"\"\"Robust text detection with adaptive confidence thresholding\"\"\"\n",
    "        results = self.easyocr_reader.readtext(np.array(image), paragraph=True)\n",
    "        regions = []\n",
    "        \n",
    "        for item in results:\n",
    "            try:\n",
    "                coords = np.array(item[0]).astype(int)\n",
    "                text = str(item[1])\n",
    "                confidence = float(item[2]) if len(item) > 2 else 0.0\n",
    "                \n",
    "                if confidence > 0.3:  # Adjusted confidence threshold\n",
    "                    regions.append({\n",
    "                        'coordinates': coords,\n",
    "                        'text': text,\n",
    "                        'confidence': confidence,\n",
    "                        'original_bbox': self.get_axis_aligned_bbox(coords)\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping invalid OCR result: {e}\")\n",
    "        \n",
    "        print(f\"Detected {len(regions)} text regions\")\n",
    "        return regions\n",
    "\n",
    "    def get_axis_aligned_bbox(self, polygon):\n",
    "        \"\"\"Convert rotated polygon to axis-aligned rectangle\"\"\"\n",
    "        x_coords = polygon[:, 0]\n",
    "        y_coords = polygon[:, 1]\n",
    "        return np.array([\n",
    "            [min(x_coords), min(y_coords)],\n",
    "            [max(x_coords), min(y_coords)],\n",
    "            [max(x_coords), max(y_coords)],\n",
    "            [min(x_coords), max(y_coords)]\n",
    "        ])\n",
    "\n",
    "    def generate_contextual_caption(self, image):\n",
    "        \"\"\"Generate image description with enhanced prompting\"\"\"\n",
    "        inputs = self.blip_processor(\n",
    "            images=image, \n",
    "            text=\"a high quality photo of\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device, torch.float16)\n",
    "        \n",
    "        generated_ids = self.blip_model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=100,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        return self.blip_processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    def correct_text_with_context(self, text, caption, image):\n",
    "        \"\"\"Context-aware text correction with validation\"\"\"\n",
    "        prompt = f\"Correct this text in image context: '{text}'. Image shows: {caption}. Correction must be:\"\n",
    "        \n",
    "        inputs = self.blip_processor(\n",
    "            images=image, \n",
    "            text=prompt,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device, torch.float16)\n",
    "        \n",
    "        generated_ids = self.blip_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            repetition_penalty=1.5,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        corrected = self.blip_processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        return corrected.strip().replace('\"', '').replace(\"'\", \"\")\n",
    "\n",
    "    def optimize_text_layout(self, regions):\n",
    "        \"\"\"Enhanced geometric optimization with visual consistency\"\"\"\n",
    "        if len(regions) < 1:\n",
    "            return regions\n",
    "\n",
    "        # Calculate initial layout parameters\n",
    "        layout_params = []\n",
    "        for region in regions:\n",
    "            bbox = region['original_bbox']\n",
    "            layout_params.append([\n",
    "                (bbox[0][0] + bbox[2][0]) / 2,  # x_center\n",
    "                (bbox[0][1] + bbox[2][1]) / 2,  # y_center\n",
    "                bbox[2][1] - bbox[0][1]          # height\n",
    "            ])\n",
    "\n",
    "        initial_params = np.array(layout_params)\n",
    "        bounds = [\n",
    "            (x-50, x+50) for x in initial_params[:, 0]\n",
    "        ] + [\n",
    "            (y-50, y+50) for y in initial_params[:, 1]\n",
    "        ] + [\n",
    "            (h*0.8, h*1.2) for h in initial_params[:, 2]\n",
    "        ]\n",
    "\n",
    "        # Run constrained optimization\n",
    "        result = minimize(\n",
    "            self.layout_energy,\n",
    "            initial_params.flatten(),\n",
    "            args=(initial_params),\n",
    "            method='L-BFGS-B',\n",
    "            bounds=bounds,\n",
    "            options={'maxiter': 200}\n",
    "        )\n",
    "\n",
    "        # Update regions with optimized layout\n",
    "        optimized = result.x.reshape(-1, 3)\n",
    "        for i, region in enumerate(regions):\n",
    "            region['optimized_bbox'] = self.create_optimized_bbox(\n",
    "                optimized[i], \n",
    "                region['original_bbox']\n",
    "            )\n",
    "        \n",
    "        return regions\n",
    "\n",
    "    def layout_energy(self, params, original):\n",
    "        \"\"\"Physics-inspired layout energy function\"\"\"\n",
    "        params = params.reshape(-1, 3)\n",
    "        energy = 0.0\n",
    "        \n",
    "        # Positional fidelity\n",
    "        energy += 0.5 * np.sum((params[:, :2] - original[:, :2])**2)\n",
    "        \n",
    "        # Size consistency\n",
    "        energy += 0.3 * np.sum((params[:, 2] - original[:, 2])**2)\n",
    "        \n",
    "        # Inter-element spacing\n",
    "        if len(params) > 1:\n",
    "            dx = np.diff(params[:, 0])\n",
    "            energy += 0.2 * np.sum((dx - np.mean(dx))**2)\n",
    "            \n",
    "        return energy\n",
    "\n",
    "    def create_optimized_bbox(self, params, original_bbox):\n",
    "        \"\"\"Create optimized bounding box from parameters\"\"\"\n",
    "        x_center, y_center, height = params\n",
    "        width = original_bbox[2][0] - original_bbox[0][0]\n",
    "        return np.array([\n",
    "            [x_center - width/2, y_center - height/2],\n",
    "            [x_center + width/2, y_center - height/2],\n",
    "            [x_center + width/2, y_center + height/2],\n",
    "            [x_center - width/2, y_center + height/2]\n",
    "        ]).astype(int)\n",
    "\n",
    "    def semantic_text_matching(self, sources, targets):\n",
    "        \"\"\"Robust text matching using combined metrics\"\"\"\n",
    "        # Create feature vectors\n",
    "        source_vecs = np.array([self.text_features(t) for t in sources])\n",
    "        target_vecs = np.array([self.text_features(t) for t in targets])\n",
    "        \n",
    "        # Combined similarity matrix\n",
    "        char_sim = cdist(source_vecs, target_vecs, 'cosine')\n",
    "        len_sim = np.abs(np.array([len(s)]*len(targets)) - np.array([len(t) for t in targets]))[:, None]\n",
    "        combined_sim = 0.7*char_sim + 0.3*len_sim/20\n",
    "        \n",
    "        # Optimal assignment\n",
    "        row_ind, col_ind = linear_sum_assignment(combined_sim)\n",
    "        return [targets[i] for i in col_ind]\n",
    "\n",
    "    def text_features(self, text):\n",
    "        \"\"\"Feature vector combining multiple text properties\"\"\"\n",
    "        text = str(text).lower()\n",
    "        return [\n",
    "            len(text),\n",
    "            *[ord(c) for c in text[:10]],\n",
    "            *[text.count(vowel) for vowel in 'aeiou']\n",
    "        ]\n",
    "\n",
    "    def generate_text_mask(self, image_size, bbox):\n",
    "        \"\"\"Precision mask generation with anti-aliasing\"\"\"\n",
    "        mask = Image.new('L', image_size, 0)\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "        draw.polygon([tuple(p) for p in bbox], fill=255)\n",
    "        return mask.resize(image_size, Image.LANCZOS)\n",
    "\n",
    "    def inpaint_text_region(self, image, mask, text):\n",
    "        \"\"\"Style-preserving inpainting with text focus\"\"\"\n",
    "        return self.inpaint_pipe(\n",
    "            prompt=f\"Professional sign with text: '{text}', perfect spelling, crisp edges, matching style\",\n",
    "            negative_prompt=\"deformed, blurry, disfigured, incorrect text, watermark\",\n",
    "            image=image,\n",
    "            mask_image=mask,\n",
    "            num_inference_steps=100,\n",
    "            guidance_scale=12.0,\n",
    "            height=image.height,\n",
    "            width=image.width\n",
    "        ).images[0]\n",
    "\n",
    "    def process_image(self, image_path):\n",
    "        \"\"\"Complete processing pipeline with validation\"\"\"\n",
    "        # Load and validate input\n",
    "        orig_image = Image.open(image_path).convert(\"RGB\")\n",
    "        print(\"\\nProcessing image:\", image_path)\n",
    "        \n",
    "        # Stage 1: Text detection\n",
    "        regions = self.detect_text_regions(orig_image)\n",
    "        if not regions:\n",
    "            print(\"No text regions found\")\n",
    "            return orig_image\n",
    "        \n",
    "        # Stage 2: Context understanding\n",
    "        caption = self.generate_contextual_caption(orig_image)\n",
    "        print(f\"Image context: {caption}\")\n",
    "        \n",
    "        # Stage 3: Text correction\n",
    "        corrected_texts = []\n",
    "        for i, region in enumerate(regions):\n",
    "            original = region['text']\n",
    "            corrected = self.correct_text_with_context(original, caption, orig_image)\n",
    "            print(f\"Region {i+1}: '{original}' → '{corrected}'\")\n",
    "            corrected_texts.append(corrected)\n",
    "        \n",
    "        # Stage 4: Text matching\n",
    "        aligned_texts = self.semantic_text_matching(\n",
    "            [r['text'] for r in regions],\n",
    "            corrected_texts\n",
    "        )\n",
    "        \n",
    "        # Stage 5: Layout optimization\n",
    "        optimized_regions = self.optimize_text_layout(regions)\n",
    "        \n",
    "        # Stage 6: Inpainting\n",
    "        result_image = orig_image.copy()\n",
    "        for region, new_text in zip(optimized_regions, aligned_texts):\n",
    "            if new_text.lower() == region['text'].lower():\n",
    "                continue\n",
    "            \n",
    "            print(f\"Processing: {region['text']} → {new_text}\")\n",
    "            try:\n",
    "                # Generate mask for current region\n",
    "                bbox = region.get('optimized_bbox', region['original_bbox'])\n",
    "                mask = self.generate_text_mask(orig_image.size, bbox)\n",
    "                \n",
    "                # Inpaint with corrected text\n",
    "                inpainted = self.inpaint_text_region(orig_image, mask, new_text)\n",
    "                result_image.paste(inpainted, (0, 0), mask)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing region: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return result_image\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    corrector = EnhancedTextCorrector()\n",
    "    result = corrector.process_image(\"Incorrect_Images/Incorrect_SOTP_sign.jpg\")\n",
    "    result.save(\"corrected_image.jpg\")\n",
    "    result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
