{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./genai/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in ./genai/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: transformers in ./genai/lib/python3.10/site-packages (4.48.3)\n",
      "Requirement already satisfied: diffusers in ./genai/lib/python3.10/site-packages (0.32.2)\n",
      "Requirement already satisfied: numpy in ./genai/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: easyocr in ./genai/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: scipy in ./genai/lib/python3.10/site-packages (1.15.1)\n",
      "Requirement already satisfied: networkx in ./genai/lib/python3.10/site-packages (3.4.2)\n",
      "Requirement already satisfied: pillow in ./genai/lib/python3.10/site-packages (11.1.0)\n",
      "Requirement already satisfied: filelock in ./genai/lib/python3.10/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./genai/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in ./genai/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./genai/lib/python3.10/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./genai/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./genai/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./genai/lib/python3.10/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./genai/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./genai/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./genai/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./genai/lib/python3.10/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./genai/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./genai/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./genai/lib/python3.10/site-packages (from transformers) (4.65.2)\n",
      "Requirement already satisfied: importlib-metadata in ./genai/lib/python3.10/site-packages (from diffusers) (8.6.1)\n",
      "Requirement already satisfied: opencv-python-headless in ./genai/lib/python3.10/site-packages (from easyocr) (4.11.0.86)\n",
      "Requirement already satisfied: scikit-image in ./genai/lib/python3.10/site-packages (from easyocr) (0.25.1)\n",
      "Requirement already satisfied: python-bidi in ./genai/lib/python3.10/site-packages (from easyocr) (0.6.3)\n",
      "Requirement already satisfied: Shapely in ./genai/lib/python3.10/site-packages (from easyocr) (2.0.7)\n",
      "Requirement already satisfied: pyclipper in ./genai/lib/python3.10/site-packages (from easyocr) (1.3.0.post6)\n",
      "Requirement already satisfied: ninja in ./genai/lib/python3.10/site-packages (from easyocr) (1.11.1.3)\n",
      "Requirement already satisfied: zipp>=3.20 in ./genai/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./genai/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./genai/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./genai/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./genai/lib/python3.10/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./genai/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in ./genai/lib/python3.10/site-packages (from scikit-image->easyocr) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./genai/lib/python3.10/site-packages (from scikit-image->easyocr) (2025.1.10)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in ./genai/lib/python3.10/site-packages (from scikit-image->easyocr) (0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision transformers diffusers numpy easyocr scipy networkx pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images...\n",
      "Images loaded successfully\n",
      "Starting evaluation...\n",
      "Extracting text from original image...\n",
      "Extracting text from corrected image...\n",
      "Calculating CER...\n",
      "Calculating WER...\n",
      "Calculating BLEU...\n",
      "Calculating SSIM...\n",
      "Calculating PSNR...\n",
      "Calculating FID...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prajitsengupta/Desktop/Correcting-Text-In-Images-Using-AI/genai/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/prajitsengupta/Desktop/Correcting-Text-In-Images-Using-AI/genai/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Not enough samples to compute covariance. Using diagonal covariance.\n",
      "Evaluation complete. Results:\n",
      "{'CER': 0.0, 'WER': 0.0, 'BLEU': 0.1778279410038923, 'SSIM': np.float64(0.9493299284518518), 'PSNR': np.float64(17.317352773547313), 'FID': np.float64(279.7239685058594), 'Original Text': 'SOTP', 'Corrected Text': 'STOP', 'Target Text': 'STOP'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.models import inception_v3\n",
    "from scipy.linalg import sqrtm\n",
    "import editdistance\n",
    "import easyocr\n",
    "\n",
    "# Image transformation for InceptionV3\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # Required input size for InceptionV3\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Initialize EasyOCR reader\n",
    "reader = easyocr.Reader(['en'])  # Initialize for English\n",
    "\n",
    "def extract_text(image):\n",
    "    result = reader.readtext(np.array(image))\n",
    "    return ' '.join([text for _, text, _ in result])\n",
    "\n",
    "def calculate_cer(reference, hypothesis):\n",
    "    return editdistance.eval(reference, hypothesis) / max(len(reference), 1)\n",
    "\n",
    "def calculate_wer(reference, hypothesis):\n",
    "    return editdistance.eval(reference.split(), hypothesis.split()) / max(len(reference.split()), 1)\n",
    "\n",
    "def calculate_bleu(reference, hypothesis):\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "    return sentence_bleu([reference.split()], hypothesis.split(), smoothing_function=smoothing_function)\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    return ssim(np.array(img1.convert(\"L\")), np.array(img2.convert(\"L\")))\n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    return psnr(np.array(img1.convert(\"L\")), np.array(img2.convert(\"L\")))\n",
    "\n",
    "def calculate_fid(real_image, generated_image):\n",
    "    inception_model = inception_v3(pretrained=True, transform_input=False).eval()\n",
    "    \n",
    "    def get_activations(image):\n",
    "        image = transform(image).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "        with torch.no_grad():\n",
    "            activations = inception_model(image)\n",
    "        return activations.cpu().numpy().squeeze()\n",
    "\n",
    "    real_activations = get_activations(real_image)\n",
    "    generated_activations = get_activations(generated_image)\n",
    "\n",
    "    # Ensure activations are 2D\n",
    "    real_activations = real_activations.reshape(1, -1)\n",
    "    generated_activations = generated_activations.reshape(1, -1)\n",
    "\n",
    "    # Replace NaN and Inf values\n",
    "    real_activations = np.nan_to_num(real_activations, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "    generated_activations = np.nan_to_num(generated_activations, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "\n",
    "    if real_activations.shape[0] > 1 and generated_activations.shape[0] > 1:\n",
    "        mu1, sigma1 = real_activations.mean(axis=0), np.cov(real_activations, rowvar=False)\n",
    "        mu2, sigma2 = generated_activations.mean(axis=0), np.cov(generated_activations, rowvar=False)\n",
    "    else:\n",
    "        # Handle the case where we don't have enough samples\n",
    "        print(\"Warning: Not enough samples to compute covariance. Using diagonal covariance.\")\n",
    "        mu1, sigma1 = real_activations.mean(axis=0), np.diag(real_activations.var(axis=0))\n",
    "        mu2, sigma2 = generated_activations.mean(axis=0), np.diag(generated_activations.var(axis=0))\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    # Handle potential numerical instabilities\n",
    "    eps = 1e-6\n",
    "    sigma1 = sigma1 + np.eye(sigma1.shape[0]) * eps\n",
    "    sigma2 = sigma2 + np.eye(sigma2.shape[0]) * eps\n",
    "\n",
    "    # Ensure no NaN or Inf values in covariance matrices\n",
    "    sigma1 = np.nan_to_num(sigma1, nan=eps, posinf=1e10, neginf=-1e10)\n",
    "    sigma2 = np.nan_to_num(sigma2, nan=eps, posinf=1e10, neginf=-1e10)\n",
    "\n",
    "    try:\n",
    "        covmean = sqrtm(sigma1.dot(sigma2))\n",
    "        if np.iscomplexobj(covmean):\n",
    "            covmean = covmean.real\n",
    "        fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n",
    "        return np.real(fid)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error in FID calculation: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_model(original_image, corrected_image, target_text):\n",
    "    results = {}\n",
    "    try:\n",
    "        print(\"Extracting text from original image...\")\n",
    "        original_text = extract_text(original_image)\n",
    "        print(\"Extracting text from corrected image...\")\n",
    "        corrected_text = extract_text(corrected_image)\n",
    "        \n",
    "        print(\"Calculating CER...\")\n",
    "        results['CER'] = calculate_cer(target_text, corrected_text)\n",
    "        print(\"Calculating WER...\")\n",
    "        results['WER'] = calculate_wer(target_text, corrected_text)\n",
    "        print(\"Calculating BLEU...\")\n",
    "        results['BLEU'] = calculate_bleu(target_text, corrected_text)\n",
    "        print(\"Calculating SSIM...\")\n",
    "        results['SSIM'] = calculate_ssim(original_image, corrected_image)\n",
    "        print(\"Calculating PSNR...\")\n",
    "        results['PSNR'] = calculate_psnr(original_image, corrected_image)\n",
    "        print(\"Calculating FID...\")\n",
    "        results['FID'] = calculate_fid(original_image, corrected_image)\n",
    "        \n",
    "        results['Original Text'] = original_text\n",
    "        results['Corrected Text'] = corrected_text\n",
    "        results['Target Text'] = target_text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    print(\"Loading images...\")\n",
    "    original_image = Image.open(\"Incorrect_Images/Incorrect_SOTP_sign.jpg\").convert(\"RGB\")\n",
    "    corrected_image = Image.open(\"test.png\").convert(\"RGB\")\n",
    "    print(\"Images loaded successfully\")\n",
    "    \n",
    "    target_text = \"STOP\"\n",
    "\n",
    "    print(\"Starting evaluation...\")\n",
    "    evaluation_results = evaluate_model(original_image, corrected_image, target_text)\n",
    "    print(\"Evaluation complete. Results:\")\n",
    "    print(evaluation_results)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred in the main execution: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
