{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "import re\n",
    "\n",
    "def compute_nld(s1, s2):\n",
    "    \"\"\"Compute Normalized Levenshtein Distance (NLD).\"\"\"\n",
    "    s1 = re.sub(r'\\s+', ' ', s1.strip().lower())\n",
    "    s2 = re.sub(r'\\s+', ' ', s2.strip().lower())\n",
    "    distance = Levenshtein.distance(s1, s2)\n",
    "    return distance / max(len(s1), len(s2))\n",
    "\n",
    "def word_level_accuracy(gt, pred):\n",
    "    \"\"\"Compute word-level accuracy and F1 score.\"\"\"\n",
    "    gt_words = gt.split()\n",
    "    pred_words = pred.split()\n",
    "    \n",
    "    common = set(gt_words) & set(pred_words)\n",
    "    accuracy = len(common) / len(gt_words) if gt_words else 0.0\n",
    "    precision = len(common) / len(pred_words) if pred_words else 0.0\n",
    "    recall = accuracy\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    return accuracy, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (\"Incorrect_Images/Incorrect_SOTP_sign.jpg\", \"STOP\"),\n",
    "    # (\"GenAI_Dataset/Imagen(Gemini)/Gemini_Generated_Image_1mqqb41mqqb41mqq.jpg\", \"LIMITLESS POSSIBBITIES\"),\n",
    "    (\"Incorrect_Images/Incorrect_Happy_Birthday.png.webp\", \"Happy Birthday\"),\n",
    "    # (\"GenAI_Dataset/Imagen(Gemini)/Gemini_Generated_Image_r8x05sr8x05sr8x0.jpg\", \"nice to met you\"),\n",
    "    # (\"Incorrect_Images/incorrect_parking.jpg\", \"No UNORTHERISED PARKING THE COMMITTEE\"),\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import networkx as nx\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "\n",
    "class AITextCorrector:\n",
    "    def __init__(self, blip_model=\"Salesforce/blip-image-captioning-base\",\n",
    "                 model_name=\"stabilityai/stable-diffusion-2-inpainting\"):\n",
    "        \"\"\"\n",
    "        Initialize models: BLIP for context-aware correction, and TextDiffuser for inpainting.\n",
    "        \"\"\"\n",
    "        # if torch.backends.mps.is_available():\n",
    "        #     self.device = torch.device(\"mps\")  # Use Apple MPS\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")  # Use CUDA if available (not applicable for Macs)\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")  # Default to CPU\n",
    "\n",
    "        print(\"Using device:\", self.device)\n",
    "\n",
    "        # Captioning - BLIP-2\n",
    "        self.blip_processor = BlipProcessor.from_pretrained(blip_model)\n",
    "        self.blip_model = BlipForConditionalGeneration.from_pretrained(blip_model).to(self.device)\n",
    "\n",
    "        # Text Inpainting - TextDiffuser\n",
    "        self.model = StableDiffusionInpaintPipeline.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "        # Traditional OCR for Bounding Box Detection\n",
    "        self.easyocr_model = easyocr.Reader(['en'])\n",
    "\n",
    "    def detect_text_boxes(self, image):\n",
    "        \"\"\"\n",
    "        Detects text regions using EasyOCR and extracts bounding boxes.\n",
    "        \"\"\"\n",
    "        image_np = np.array(image)\n",
    "        ocr_results = self.easyocr_model.readtext(image_np)\n",
    "        return [{\"coordinates\": result[0], \"text\": result[1]} for result in ocr_results]\n",
    "\n",
    "    def recognize_text(self, image):\n",
    "        \"\"\"\n",
    "        Recognizes text in the image using EasyOCR.\n",
    "        \"\"\"\n",
    "        image_np = np.array(image)\n",
    "        ocr_results = self.easyocr_model.readtext(image_np)\n",
    "\n",
    "        recognized_text = \" \".join([result[1] for result in ocr_results])  # Join detected text pieces\n",
    "        print(f\"OCR Output: {recognized_text}\")\n",
    "        return recognized_text\n",
    "\n",
    "    def generate_caption(self, image):\n",
    "        \"\"\"\n",
    "        Generates a descriptive caption for the image using BLIP-2.\n",
    "        \"\"\"\n",
    "        inputs = self.blip_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs[\"pixel_values\"]  # Extract pixel values\n",
    "        with torch.no_grad():\n",
    "            outputs = self.blip_model.generate(pixel_values=pixel_values)  # Pass pixel_values explicitly\n",
    "        return self.blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def correct_text(self, extracted_text, caption, image):\n",
    "        \"\"\"\n",
    "        Uses BLIP-2 to refine extracted text based on image caption context.\n",
    "        \"\"\"\n",
    "        inputs = self.blip_processor(images=image, text=f\"Correct this text: {extracted_text} in context: {caption}\", return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs[\"pixel_values\"]  # Extract pixel values\n",
    "        with torch.no_grad():\n",
    "            outputs = self.blip_model.generate(pixel_values=pixel_values)  # âœ… Pass explicitly\n",
    "        return self.blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def create_mask(self, image_size, coordinates):\n",
    "        \"\"\"\n",
    "        Creates a binary mask for the text regions.\n",
    "        \"\"\"\n",
    "        mask = Image.new('L', image_size, 0)\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "        draw.polygon([tuple(point) for point in coordinates], outline=255, fill=255)\n",
    "        return mask\n",
    "\n",
    "    def graph_based_text_alignment(self, detected_boxes):\n",
    "        \"\"\"\n",
    "        Uses a graph-based Hungarian Matching algorithm to align detected text positions.\n",
    "        \"\"\"\n",
    "        num_boxes = len(detected_boxes)\n",
    "        cost_matrix = np.zeros((num_boxes, num_boxes))\n",
    "\n",
    "        for i in range(num_boxes):\n",
    "            for j in range(num_boxes):\n",
    "                if i != j:\n",
    "                    # Distance-based cost function\n",
    "                    x1, y1 = np.mean(detected_boxes[i]['coordinates'], axis=0)\n",
    "                    x2, y2 = np.mean(detected_boxes[j]['coordinates'], axis=0)\n",
    "                    cost_matrix[i, j] = np.linalg.norm(np.array([x1, y1]) - np.array([x2, y2]))\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        aligned_boxes = [detected_boxes[i] for i in row_ind]\n",
    "        return aligned_boxes\n",
    "\n",
    "    def inpaint_text(self, image, mask, corrected_text):\n",
    "        \"\"\"\n",
    "        Inpaints the corrected text using TextDiffuser.\n",
    "        \"\"\"\n",
    "        return self.model(prompt=f\"Generate text '{corrected_text}' in a matching style\", image=image, mask_image=mask, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
    "\n",
    "    def run_pipeline(self, image):\n",
    "        \"\"\"\n",
    "        Runs the complete text correction pipeline.\n",
    "        \"\"\"\n",
    "        text_boxes = self.detect_text_boxes(image)\n",
    "        caption = self.generate_caption(image)\n",
    "        aligned_boxes = self.graph_based_text_alignment(text_boxes)\n",
    "\n",
    "        corrected_image = image.copy()\n",
    "\n",
    "        for box in aligned_boxes:\n",
    "            original_text = box[\"text\"]\n",
    "            corrected_text = self.correct_text(original_text, caption, image)\n",
    "\n",
    "            if corrected_text.strip() == original_text.strip():\n",
    "                continue  # Skip if no correction needed\n",
    "\n",
    "            mask = self.create_mask(image.size, box[\"coordinates\"])\n",
    "            mask = mask.resize(image.size)  # Ensure mask is the same size as image\n",
    "            inpainted_region = self.inpaint_text(corrected_image, mask, corrected_text)\n",
    "\n",
    "            # Blend the corrected text back into the image\n",
    "            full_mask = Image.new('L', corrected_image.size, 0)\n",
    "            full_mask.paste(mask, (0, 0))\n",
    "            mask = mask.resize(image.size, Image.LANCZOS)\n",
    "\n",
    "            # Ensure the inpainted region is the same size as the mask\n",
    "            inpainted_region = inpainted_region.resize(image.size, Image.LANCZOS)\n",
    "\n",
    "            # Debugging: Print sizes before pasting\n",
    "            print(\"Original image size:\", image.size)\n",
    "            print(\"Mask size:\", mask.size)\n",
    "            print(\"Inpainted region size:\", inpainted_region.size)\n",
    "\n",
    "            # Paste the inpainted region back into the corrected image\n",
    "            corrected_image.paste(inpainted_region, (0, 0), mask)\n",
    "\n",
    "        return corrected_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AITextCorrectorAblation(AITextCorrector):\n",
    "    def run_pipeline_ablation(self, image, use_simulated_annealing=True, use_ocr_inpainting=True):\n",
    "        \"\"\"\n",
    "        Run the pipeline with options to disable simulated annealing or OCR inpainting.\n",
    "        \"\"\"\n",
    "        text_boxes = self.detect_text_boxes(image)\n",
    "        caption = self.generate_caption(image)\n",
    "\n",
    "        # If Simulated Annealing is disabled, skip the alignment step\n",
    "        if not use_simulated_annealing:\n",
    "            aligned_boxes = text_boxes  # Don't perform alignment if Simulated Annealing is disabled\n",
    "        else:\n",
    "            aligned_boxes = self.graph_based_text_alignment(text_boxes)\n",
    "\n",
    "        corrected_image = image.copy()\n",
    "\n",
    "        for box in aligned_boxes:\n",
    "            original_text = box[\"text\"]\n",
    "            corrected_text = self.correct_text(original_text, caption, image)\n",
    "\n",
    "            if corrected_text.strip() == original_text.strip():\n",
    "                continue  # Skip if no correction needed\n",
    "\n",
    "            if use_ocr_inpainting:\n",
    "                # Create the mask and ensure it matches the size of the image\n",
    "                mask = self.create_mask(image.size, box[\"coordinates\"])\n",
    "                mask = mask.resize(image.size, Image.LANCZOS)  # Resize mask to match image size\n",
    "\n",
    "                # Inpaint the corrected text\n",
    "                inpainted_region = self.inpaint_text(corrected_image, mask, corrected_text)\n",
    "\n",
    "                # Resize the inpainted region to match the image size\n",
    "                inpainted_region = inpainted_region.resize(image.size, Image.LANCZOS)\n",
    "\n",
    "                # Ensure the mask is resized to the image size as well\n",
    "                full_mask = Image.new('L', corrected_image.size, 0)\n",
    "                full_mask.paste(mask, (0, 0))\n",
    "\n",
    "                # Debugging: Print sizes before pasting\n",
    "                print(\"Original image size:\", image.size)\n",
    "                print(\"Mask size:\", mask.size)\n",
    "                print(\"Inpainted region size:\", inpainted_region.size)\n",
    "\n",
    "                # Paste the inpainted region back into the image using the mask\n",
    "                corrected_image.paste(inpainted_region, (0, 0), full_mask)  # Ensure mask is applied correctly\n",
    "\n",
    "        return corrected_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ac736e697945eaac9244d203a2b6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCR Output: SOTP\n",
      "OCR Output: HAPPP  Hanpdday Birthday\n",
      "OCR Output: SOTP\n",
      "OCR Output: HAPPP  Hanpdday Birthday\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beacec28a57e43aa996a4c3fbb20ef06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: (2816, 2112)\n",
      "Mask size: (2816, 2112)\n",
      "Inpainted region size: (2816, 2112)\n",
      "OCR Output: STOP\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a169ee8f0143464688f1686ff520a611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: (512, 512)\n",
      "Mask size: (512, 512)\n",
      "Inpainted region size: (512, 512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9944bdf48445c59a2bc8a60693d3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: (512, 512)\n",
      "Mask size: (512, 512)\n",
      "Inpainted region size: (512, 512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2746c814729d45a99c1d2c63022d90a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: (512, 512)\n",
      "Mask size: (512, 512)\n",
      "Inpainted region size: (512, 512)\n",
      "OCR Output: Happpy BitR Hdday\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f179f5a38a24412cbe2af6f52dce2c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: (2816, 2112)\n",
      "Mask size: (2816, 2112)\n",
      "Inpainted region size: (2816, 2112)\n",
      "OCR Output: S7ITOP\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9547b0ce1a545b78d45e8a7be0b8073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: (512, 512)\n",
      "Mask size: (512, 512)\n",
      "Inpainted region size: (512, 512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82a4ca24aaa4b579c909c6846216f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: (512, 512)\n",
      "Mask size: (512, 512)\n",
      "Inpainted region size: (512, 512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f88a05fbd794f7dafef5be0edb1d476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image size: (512, 512)\n",
      "Mask size: (512, 512)\n",
      "Inpainted region size: (512, 512)\n",
      "OCR Output: Hopppv HAppPY BIRTHDDAY\n",
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "Method & Avg F1 Score \\\\\n",
      "\\midrule\n",
      "Baseline (No Correction) & 0.200000 \\\\\n",
      "With Simulated Annealing & 0.200000 \\\\\n",
      "With OCR In-painting & 0.500000 \\\\\n",
      "Full Model & 0.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ablation_conditions = [\n",
    "    {\"name\": \"Baseline (No Correction)\", \"simulated\": False, \"ocr\": False},\n",
    "    {\"name\": \"With Simulated Annealing\", \"simulated\": True, \"ocr\": False},\n",
    "    {\"name\": \"With OCR In-painting\", \"simulated\": False, \"ocr\": True},\n",
    "    {\"name\": \"Full Model\", \"simulated\": True, \"ocr\": True}\n",
    "]\n",
    "\n",
    "results_ablation = []\n",
    "corrector_ablation = AITextCorrectorAblation()\n",
    "\n",
    "for cond in ablation_conditions:\n",
    "    metrics = []\n",
    "    for img_path, gt_text in dataset:  # 'dataset' should be your test dataset with images and ground truth\n",
    "        image = Image.open(img_path)\n",
    "        corrected_image = corrector_ablation.run_pipeline_ablation(\n",
    "            image, use_simulated_annealing=cond[\"simulated\"], use_ocr_inpainting=cond[\"ocr\"]\n",
    "        )\n",
    "        pred_text = corrector_ablation.recognize_text(corrected_image)\n",
    "        _, f1 = word_level_accuracy(gt_text, pred_text)  # You may need to adjust the word_level_accuracy method\n",
    "        metrics.append(f1)\n",
    "    avg_f1 = sum(metrics) / len(metrics)\n",
    "    results_ablation.append({\"Method\": cond[\"name\"], \"Avg F1 Score\": round(avg_f1, 2)})\n",
    "\n",
    "df_ablation = pd.DataFrame(results_ablation)\n",
    "print(df_ablation.to_latex(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "Method & Avg F1 Score \\\\\n",
      "\\midrule\n",
      "Baseline (No Correction) & 0.200000 \\\\\n",
      "With Simulated Annealing & 0.200000 \\\\\n",
      "With OCR In-painting & 0.500000 \\\\\n",
      "Full Model & 0.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_ablation = pd.DataFrame(results_ablation)\n",
    "print(df_ablation.to_latex(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
