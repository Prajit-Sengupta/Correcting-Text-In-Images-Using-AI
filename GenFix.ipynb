{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./genai/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in ./genai/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: transformers in ./genai/lib/python3.10/site-packages (4.48.3)\n",
      "Requirement already satisfied: diffusers in ./genai/lib/python3.10/site-packages (0.32.2)\n",
      "Requirement already satisfied: numpy in ./genai/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: easyocr in ./genai/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: scipy in ./genai/lib/python3.10/site-packages (1.15.1)\n",
      "Requirement already satisfied: networkx in ./genai/lib/python3.10/site-packages (3.4.2)\n",
      "Requirement already satisfied: pillow in ./genai/lib/python3.10/site-packages (11.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./genai/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in ./genai/lib/python3.10/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./genai/lib/python3.10/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./genai/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: filelock in ./genai/lib/python3.10/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./genai/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./genai/lib/python3.10/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./genai/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./genai/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./genai/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./genai/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./genai/lib/python3.10/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: requests in ./genai/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./genai/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: importlib-metadata in ./genai/lib/python3.10/site-packages (from diffusers) (8.6.1)\n",
      "Requirement already satisfied: opencv-python-headless in ./genai/lib/python3.10/site-packages (from easyocr) (4.11.0.86)\n",
      "Requirement already satisfied: pyclipper in ./genai/lib/python3.10/site-packages (from easyocr) (1.3.0.post6)\n",
      "Requirement already satisfied: ninja in ./genai/lib/python3.10/site-packages (from easyocr) (1.11.1.3)\n",
      "Requirement already satisfied: scikit-image in ./genai/lib/python3.10/site-packages (from easyocr) (0.25.1)\n",
      "Requirement already satisfied: Shapely in ./genai/lib/python3.10/site-packages (from easyocr) (2.0.7)\n",
      "Requirement already satisfied: python-bidi in ./genai/lib/python3.10/site-packages (from easyocr) (0.6.3)\n",
      "Requirement already satisfied: zipp>=3.20 in ./genai/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./genai/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./genai/lib/python3.10/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./genai/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./genai/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./genai/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in ./genai/lib/python3.10/site-packages (from scikit-image->easyocr) (0.4)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in ./genai/lib/python3.10/site-packages (from scikit-image->easyocr) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./genai/lib/python3.10/site-packages (from scikit-image->easyocr) (2025.1.10)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision transformers diffusers numpy easyocr scipy networkx pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489cb32c5f534e86857e8f59a5e62ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.48.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 1024,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Loading pipeline components...: 100%|██████████| 6/6 [00:13<00:00,  2.32s/it]\n",
      "100%|██████████| 50/50 [06:08<00:00,  7.37s/it]\n",
      "100%|██████████| 50/50 [06:00<00:00,  7.20s/it]\n",
      "100%|██████████| 50/50 [06:04<00:00,  7.29s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, BlipProcessor, BlipForConditionalGeneration\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import networkx as nx\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class AITextCorrector:\n",
    "    def __init__(self, trocr_model=\"microsoft/trocr-large-handwritten\",\n",
    "                 blip_model=\"Salesforce/blip-image-captioning-base\",\n",
    "                 model_name=\"stabilityai/stable-diffusion-2-inpainting\"):\n",
    "        \"\"\"\n",
    "        Initialize models: TrOCR for OCR, BLIP for context-aware correction, and TextDiffuser for inpainting.\n",
    "        \"\"\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # OCR - TrOCR\n",
    "        self.ocr_processor = TrOCRProcessor.from_pretrained(trocr_model)\n",
    "        self.ocr_model = VisionEncoderDecoderModel.from_pretrained(trocr_model).to(self.device)\n",
    "\n",
    "        # Captioning - BLIP-2\n",
    "        self.blip_processor = BlipProcessor.from_pretrained(blip_model)\n",
    "        self.blip_model = BlipForConditionalGeneration.from_pretrained(blip_model).to(self.device)\n",
    "\n",
    "        # Text Inpainting - TextDiffuser\n",
    "        self.model = StableDiffusionInpaintPipeline.from_pretrained(model_name).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Traditional OCR for Bounding Box Detection\n",
    "        self.easyocr_model = easyocr.Reader(['en'])\n",
    "\n",
    "    def detect_text_boxes(self, image):\n",
    "        \"\"\"\n",
    "        Detects text regions using EasyOCR and extracts bounding boxes.\n",
    "        \"\"\"\n",
    "        image_np = np.array(image)\n",
    "        ocr_results = self.easyocr_model.readtext(image_np)\n",
    "        return [{\"coordinates\": result[0], \"text\": result[1]} for result in ocr_results]\n",
    "\n",
    "    def recognize_text(self, image):\n",
    "        \"\"\"\n",
    "        Recognizes text in the image using TrOCR.\n",
    "        \"\"\"\n",
    "        pixel_values = self.ocr_processor(images=image, return_tensors=\"pt\").pixel_values.to(self.device)\n",
    "        generated_ids = self.ocr_model.generate(pixel_values)\n",
    "        return self.ocr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    def generate_caption(self, image):\n",
    "        \"\"\"\n",
    "        Generates a descriptive caption for the image using BLIP-2.\n",
    "        \"\"\"\n",
    "        inputs = self.blip_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs[\"pixel_values\"]  # Extract pixel values\n",
    "        with torch.no_grad():\n",
    "            outputs = self.blip_model.generate(pixel_values=pixel_values)  # Pass pixel_values explicitly\n",
    "        return self.blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def correct_text(self, extracted_text, caption, image):\n",
    "        \"\"\"\n",
    "        Uses BLIP-2 to refine extracted text based on image caption context.\n",
    "        \"\"\"\n",
    "        inputs = self.blip_processor(images=image, text=f\"Correct this text: {extracted_text} in context: {caption}\", return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs[\"pixel_values\"]  # Extract pixel values\n",
    "        with torch.no_grad():\n",
    "            outputs = self.blip_model.generate(pixel_values=pixel_values)  # ✅ Pass explicitly\n",
    "        return self.blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    def create_mask(self, image_size, coordinates):\n",
    "        \"\"\"\n",
    "        Creates a binary mask for the text regions.\n",
    "        \"\"\"\n",
    "        mask = Image.new('L', image_size, 0)\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "        draw.polygon([tuple(point) for point in coordinates], outline=255, fill=255)\n",
    "        return mask\n",
    "\n",
    "    def graph_based_text_alignment(self, detected_boxes):\n",
    "        \"\"\"\n",
    "        Uses a graph-based Hungarian Matching algorithm to align detected text positions.\n",
    "        \"\"\"\n",
    "        num_boxes = len(detected_boxes)\n",
    "        cost_matrix = np.zeros((num_boxes, num_boxes))\n",
    "\n",
    "        for i in range(num_boxes):\n",
    "            for j in range(num_boxes):\n",
    "                if i != j:\n",
    "                    # Distance-based cost function\n",
    "                    x1, y1 = np.mean(detected_boxes[i]['coordinates'], axis=0)\n",
    "                    x2, y2 = np.mean(detected_boxes[j]['coordinates'], axis=0)\n",
    "                    cost_matrix[i, j] = np.linalg.norm(np.array([x1, y1]) - np.array([x2, y2]))\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        aligned_boxes = [detected_boxes[i] for i in row_ind]\n",
    "        return aligned_boxes\n",
    "\n",
    "    def inpaint_text(self, image, mask, corrected_text):\n",
    "        \"\"\"\n",
    "        Inpaints the corrected text using TextDiffuser.\n",
    "        \"\"\"\n",
    "        return self.model(prompt=f\"Generate text '{corrected_text}' in a matching style\", image=image, mask_image=mask, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
    "\n",
    "    def run_pipeline(self, image):\n",
    "        \"\"\"\n",
    "        Runs the complete text correction pipeline.\n",
    "        \"\"\"\n",
    "        text_boxes = self.detect_text_boxes(image)\n",
    "        caption = self.generate_caption(image)\n",
    "        aligned_boxes = self.graph_based_text_alignment(text_boxes)\n",
    "\n",
    "        corrected_image = image.copy()\n",
    "\n",
    "        for box in aligned_boxes:\n",
    "            original_text = box[\"text\"]\n",
    "            corrected_text = self.correct_text(original_text, caption, image)\n",
    "\n",
    "            if corrected_text.strip() == original_text.strip():\n",
    "                continue  # Skip if no correction needed\n",
    "\n",
    "            mask = self.create_mask(image.size, box[\"coordinates\"])\n",
    "            inpainted_region = self.inpaint_text(corrected_image, mask, corrected_text)\n",
    "\n",
    "            # Blend the corrected text back into the image\n",
    "            full_mask = Image.new('L', corrected_image.size, 0)\n",
    "            full_mask.paste(mask, (0, 0))\n",
    "            corrected_image.paste(inpainted_region, (0, 0), full_mask)\n",
    "\n",
    "        return corrected_image\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    corrector = AITextCorrector()\n",
    "\n",
    "    input_image = Image.open(\"Incorrect_Happy_Birthday.png.jpg\")  # Replace with your test image\n",
    "    output_image = corrector.run_pipeline(input_image)\n",
    "    output_image.save(\"corrected_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
