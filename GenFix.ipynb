{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision transformers diffusers numpy easyocr scipy networkx pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4f3a22dab945d9bee5fe013e93acef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.48.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 1024,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68751e23ca74c27aec32c71ab1f27fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e75a6b8ed64246bc5667411e34564d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, BlipProcessor, BlipForConditionalGeneration\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import networkx as nx\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from accelerate import infer_auto_device_map, dispatch_model\n",
    "\n",
    "class AITextCorrector:\n",
    "    def __init__(self, trocr_model=\"microsoft/trocr-large-handwritten\",\n",
    "                 blip_model=\"Salesforce/blip-image-captioning-base\",\n",
    "                 model_name=\"stabilityai/stable-diffusion-2-inpainting\"):\n",
    "        \"\"\"\n",
    "        Initialize models: TrOCR for OCR, BLIP for context-aware correction, and TextDiffuser for inpainting.\n",
    "        \"\"\"\n",
    "        # if torch.backends.mps.is_available():\n",
    "        #     self.device = torch.device(\"mps\")  # Use Apple MPS\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")  # Use CUDA if available (not applicable for Macs)\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")  # Default to CPU\n",
    "\n",
    "        print(\"Using device:\", self.device)\n",
    "\n",
    "        # OCR - TrOCR\n",
    "        self.ocr_processor = TrOCRProcessor.from_pretrained(trocr_model)\n",
    "        # self.ocr_model = VisionEncoderDecoderModel.from_pretrained(trocr_model).to(self.device)\n",
    "\n",
    "        # First, load the model\n",
    "        self.ocr_model = VisionEncoderDecoderModel.from_pretrained(trocr_model)\n",
    "\n",
    "        # Then, infer the device map\n",
    "        device_map = infer_auto_device_map(self.ocr_model)\n",
    "\n",
    "        # Finally, move the model to the computed device\n",
    "        self.ocr_model = dispatch_model(self.ocr_model, device_map=device_map)\n",
    "\n",
    "        # Captioning - BLIP-2\n",
    "        self.blip_processor = BlipProcessor.from_pretrained(blip_model)\n",
    "        self.blip_model = BlipForConditionalGeneration.from_pretrained(blip_model).to(self.device)\n",
    "\n",
    "        # Text Inpainting - TextDiffuser\n",
    "        self.model = StableDiffusionInpaintPipeline.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "        # Traditional OCR for Bounding Box Detection\n",
    "        self.easyocr_model = easyocr.Reader(['en'])\n",
    "\n",
    "    def detect_text_boxes(self, image):\n",
    "        \"\"\"\n",
    "        Detects text regions using EasyOCR and extracts bounding boxes.\n",
    "        \"\"\"\n",
    "        image_np = np.array(image)\n",
    "        ocr_results = self.easyocr_model.readtext(image_np)\n",
    "        return [{\"coordinates\": result[0], \"text\": result[1]} for result in ocr_results]\n",
    "\n",
    "    def recognize_text(self, image):\n",
    "        \"\"\"\n",
    "        Recognizes text in the image using TrOCR.\n",
    "        \"\"\"\n",
    "        pixel_values = self.ocr_processor(images=image, return_tensors=\"pt\").pixel_values.to(self.device)\n",
    "        generated_ids = self.ocr_model.generate(pixel_values)\n",
    "        return self.ocr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    def generate_caption(self, image):\n",
    "        \"\"\"\n",
    "        Generates a descriptive caption for the image using BLIP-2.\n",
    "        \"\"\"\n",
    "        inputs = self.blip_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs[\"pixel_values\"]  # Extract pixel values\n",
    "        with torch.no_grad():\n",
    "            outputs = self.blip_model.generate(pixel_values=pixel_values)  # Pass pixel_values explicitly\n",
    "        return self.blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    def correct_text(self, extracted_text, caption, image):\n",
    "        \"\"\"\n",
    "        Uses BLIP-2 to refine extracted text based on image caption context.\n",
    "        \"\"\"\n",
    "        inputs = self.blip_processor(images=image, text=f\"Correct this text: {extracted_text} in context: {caption}\", return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs[\"pixel_values\"]  # Extract pixel values\n",
    "        with torch.no_grad():\n",
    "            outputs = self.blip_model.generate(pixel_values=pixel_values)  # ✅ Pass explicitly\n",
    "        return self.blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    def create_mask(self, image_size, coordinates):\n",
    "        \"\"\"\n",
    "        Creates a binary mask for the text regions.\n",
    "        \"\"\"\n",
    "        mask = Image.new('L', image_size, 0)\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "        draw.polygon([tuple(point) for point in coordinates], outline=255, fill=255)\n",
    "        return mask\n",
    "\n",
    "    def graph_based_text_alignment(self, detected_boxes):\n",
    "        \"\"\"\n",
    "        Uses a graph-based Hungarian Matching algorithm to align detected text positions.\n",
    "        \"\"\"\n",
    "        num_boxes = len(detected_boxes)\n",
    "        cost_matrix = np.zeros((num_boxes, num_boxes))\n",
    "\n",
    "        for i in range(num_boxes):\n",
    "            for j in range(num_boxes):\n",
    "                if i != j:\n",
    "                    # Distance-based cost function\n",
    "                    x1, y1 = np.mean(detected_boxes[i]['coordinates'], axis=0)\n",
    "                    x2, y2 = np.mean(detected_boxes[j]['coordinates'], axis=0)\n",
    "                    cost_matrix[i, j] = np.linalg.norm(np.array([x1, y1]) - np.array([x2, y2]))\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        aligned_boxes = [detected_boxes[i] for i in row_ind]\n",
    "        return aligned_boxes \n",
    "\n",
    "    def inpaint_text(self, image, mask, corrected_text):\n",
    "        \"\"\"\n",
    "        Inpaints the corrected text using TextDiffuser.\n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "        return self.model(prompt=f\"Generate text '{corrected_text}' in a matching style\", image=image, mask_image=mask, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
    "\n",
    "    def run_pipeline(self, image):\n",
    "        \"\"\"\n",
    "        Runs the complete text correction pipeline.\n",
    "        \"\"\"\n",
    "        text_boxes = self.detect_text_boxes(image)\n",
    "        caption = self.generate_caption(image)\n",
    "        aligned_boxes = self.graph_based_text_alignment(text_boxes)\n",
    "\n",
    "        corrected_image = image.copy()\n",
    "\n",
    "        for box in aligned_boxes:\n",
    "            original_text = box[\"text\"]\n",
    "            corrected_text = self.correct_text(original_text, caption, image)\n",
    "\n",
    "            if corrected_text.strip() == original_text.strip():\n",
    "                continue  # Skip if no correction needed\n",
    "            \n",
    "            # mask = self.create_mask(image.size, box[\"coordinates\"])\n",
    "            \n",
    "            mask = self.create_mask(image.size, box[\"coordinates\"])\n",
    "            mask = mask.resize(image.size)  # Ensure mask is the same size as image\n",
    "            inpainted_region = self.inpaint_text(corrected_image, mask, corrected_text)\n",
    "\n",
    "            # Blend the corrected text back into the image\n",
    "            full_mask = Image.new('L', corrected_image.size, 0)\n",
    "            full_mask.paste(mask, (0, 0))\n",
    "            # corrected_image.paste(inpainted_region, (0, 0), full_mask)\n",
    "            # Ensure the mask is resized to match the original image\n",
    "            mask = mask.resize(image.size, Image.LANCZOS)\n",
    "\n",
    "            # Ensure the inpainted region is the same size as the mask\n",
    "            inpainted_region = inpainted_region.resize(image.size, Image.LANCZOS)\n",
    "\n",
    "            # Debugging: Print sizes before pasting\n",
    "            print(\"Original image size:\", image.size)\n",
    "            print(\"Mask size:\", mask.size)\n",
    "            print(\"Inpainted region size:\", inpainted_region.size)\n",
    "\n",
    "            # Paste the inpainted region back into the corrected image\n",
    "            corrected_image.paste(inpainted_region, (0, 0), mask)\n",
    "\n",
    "        return corrected_image\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    corrector = AITextCorrector()\n",
    "\n",
    "    input_image = Image.open(\"Incorrect_Images/Incorrect_SOTP_sign.jpg\")  # Replace with your test image\n",
    "    output_image = corrector.run_pipeline(input_image)\n",
    "    print(\"Corrected Image Generated\")\n",
    "    output_image.save(\"test.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free unused memory on MPS\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Run garbage collection to free memory\n",
    "gc.collect()\n",
    "\n",
    "# Free unused memory on MPS\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-Levenshtein\n",
      "  Downloading python_Levenshtein-0.26.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting Levenshtein==0.26.1 (from python-Levenshtein)\n",
      "  Downloading levenshtein-0.26.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in ./genai/lib/python3.10/site-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.12.1)\n",
      "Downloading python_Levenshtein-0.26.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading levenshtein-0.26.1-cp310-cp310-macosx_11_0_arm64.whl (157 kB)\n",
      "Installing collected packages: Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.26.1 python-Levenshtein-0.26.1\n"
     ]
    }
   ],
   "source": [
    "# For Levenshtein distance\n",
    "!pip install python-Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"transformers_version\": \"4.48.3\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 1024,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": true,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.48.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-large-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70d8103d312429d8633661669127e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9019abe811ce4759ad45323297f8880d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3cdd777af54c0da8c643001b24f17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59615c8ad1fd433c9ec6cc2ca5082294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Graph + Energy Optimzation Correction\n",
    "import torch\n",
    "import gc\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, BlipProcessor, BlipForConditionalGeneration\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import easyocr\n",
    "import networkx as nx\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class AITextCorrector:\n",
    "    def __init__(self, trocr_model=\"microsoft/trocr-large-handwritten\",\n",
    "                 blip_model=\"Salesforce/blip-image-captioning-base\",\n",
    "                 model_name=\"stabilityai/stable-diffusion-2-inpainting\"):\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"Using device:\", self.device)\n",
    "        \n",
    "        # OCR - TrOCR\n",
    "        self.ocr_processor = TrOCRProcessor.from_pretrained(trocr_model)\n",
    "        self.ocr_model = VisionEncoderDecoderModel.from_pretrained(trocr_model).to(self.device)\n",
    "        \n",
    "        # Captioning - BLIP\n",
    "        self.blip_processor = BlipProcessor.from_pretrained(blip_model)\n",
    "        self.blip_model = BlipForConditionalGeneration.from_pretrained(blip_model).to(self.device)\n",
    "        \n",
    "        # Text Inpainting - Stable Diffusion\n",
    "        self.model = StableDiffusionInpaintPipeline.from_pretrained(model_name).to(self.device)\n",
    "        \n",
    "        # Traditional OCR for Bounding Box Detection\n",
    "        self.easyocr_model = easyocr.Reader(['en'])\n",
    "    \n",
    "    def detect_text_boxes(self, image):\n",
    "        ocr_results = self.easyocr_model.readtext(np.array(image))\n",
    "        return [{\"coordinates\": result[0], \"text\": result[1]} for result in ocr_results]\n",
    "    \n",
    "    def recognize_text(self, image):\n",
    "        pixel_values = self.ocr_processor(images=image, return_tensors=\"pt\").pixel_values.to(self.device)\n",
    "        generated_ids = self.ocr_model.generate(pixel_values)\n",
    "        return self.ocr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    def generate_caption(self, image):\n",
    "        inputs = self.blip_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.blip_model.generate(pixel_values=inputs[\"pixel_values\"])\n",
    "        return self.blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def correct_text(self, extracted_text, caption, image):\n",
    "        inputs = self.blip_processor(images=image, text=f\"Correct this text: {extracted_text} in context: {caption}\", return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.blip_model.generate(pixel_values=inputs[\"pixel_values\"])\n",
    "        return self.blip_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    def graph_based_text_alignment(self, detected_boxes):\n",
    "        num_boxes = len(detected_boxes)\n",
    "        cost_matrix = np.zeros((num_boxes, num_boxes))\n",
    "        for i in range(num_boxes):\n",
    "            for j in range(num_boxes):\n",
    "                if i != j:\n",
    "                    x1, y1 = np.mean(detected_boxes[i]['coordinates'], axis=0)\n",
    "                    x2, y2 = np.mean(detected_boxes[j]['coordinates'], axis=0)\n",
    "                    cost_matrix[i, j] = np.linalg.norm(np.array([x1, y1]) - np.array([x2, y2]))\n",
    "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "        return [detected_boxes[i] for i in row_ind]\n",
    "    \n",
    "    def energy_based_optimization(self, aligned_boxes):\n",
    "        optimized_boxes = []\n",
    "        for box in aligned_boxes:\n",
    "            box[\"coordinates\"] = np.array(box[\"coordinates\"]) + np.random.uniform(-1, 1, (4, 2))  # Small adjustment\n",
    "            optimized_boxes.append(box)\n",
    "        return optimized_boxes\n",
    "    \n",
    "    def inpaint_text(self, image, mask, corrected_text):\n",
    "        return self.model(prompt=f\"Generate text '{corrected_text}' in a matching style\", image=image, mask_image=mask, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
    "    \n",
    "    def run_pipeline(self, image):\n",
    "        text_boxes = self.detect_text_boxes(image)\n",
    "        caption = self.generate_caption(image)\n",
    "        aligned_boxes = self.graph_based_text_alignment(text_boxes)\n",
    "        optimized_boxes = self.energy_based_optimization(aligned_boxes)\n",
    "        corrected_image = image.copy()\n",
    "\n",
    "        for box in optimized_boxes:\n",
    "            original_text = box[\"text\"]\n",
    "            corrected_text = self.correct_text(original_text, caption, image)\n",
    "\n",
    "            if corrected_text.strip() == original_text.strip():\n",
    "                continue  # Skip if no correction needed\n",
    "\n",
    "            mask = Image.new('L', image.size, 0)\n",
    "            draw = ImageDraw.Draw(mask)\n",
    "            draw.polygon([tuple(point) for point in box[\"coordinates\"]], outline=255, fill=255)\n",
    "            mask = mask.resize(image.size, Image.LANCZOS)\n",
    "\n",
    "            inpainted_region = self.inpaint_text(corrected_image, mask, corrected_text)\n",
    "            inpainted_region = inpainted_region.resize(image.size, Image.LANCZOS)\n",
    "\n",
    "            print(\"Original image size:\", image.size)\n",
    "            print(\"Mask size:\", mask.size)\n",
    "            print(\"Inpainted region size:\", inpainted_region.size)\n",
    "\n",
    "            corrected_image.paste(inpainted_region, (0, 0), mask)\n",
    "\n",
    "        return corrected_image\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    corrector = AITextCorrector()\n",
    "    input_image = Image.open(\"Incorrect_Images/Incorrect_Happy_Birthday.png.webp\")  \n",
    "    output_image = corrector.run_pipeline(input_image)\n",
    "    output_image.save(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
